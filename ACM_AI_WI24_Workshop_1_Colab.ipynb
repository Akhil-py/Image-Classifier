{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhil-py/Image-Classifier/blob/main/ACM_AI_WI24_Workshop_1_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACM AI Winter 2024 Workshop #1: SVM, Image Classification"
      ],
      "metadata": {
        "id": "QeCSwYyukqGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ3qgGaYjbVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from torchvision.datasets.utils import download_url\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage as ski\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download CIFAR-100"
      ],
      "metadata": {
        "id": "QRDIdZl7kWUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz\"\n",
        "download_url(dataset_url, '.')"
      ],
      "metadata": {
        "id": "KrjjPz3rjoTQ",
        "outputId": "64c6dd7f-10a2-42d5-b73c-647d85f67065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'download_url' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac2e33aa2af9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dowload the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'download_url' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract from archive\n",
        "with tarfile.open('./cifar100.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "data_dir = './data/cifar100'"
      ],
      "metadata": {
        "id": "PPRZAolCjvKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that in the data/cifar100 folder. We have our **train** and **test** sets. In each sets, we have 20 major categories."
      ],
      "metadata": {
        "id": "6yEcAPXfuDc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Superclass\n",
        "superclasses = os.listdir(os.path.join(data_dir, 'train'))\n",
        "print(\"Superclasses:\")\n",
        "for c in superclasses:\n",
        "  print(\"-\", c)"
      ],
      "metadata": {
        "id": "NPpOKtngkMwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose a superclass you like"
      ],
      "metadata": {
        "id": "hAdKQ5uWurCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a superclass as your dataset\n",
        "cat = \"your_choice\""
      ],
      "metadata": {
        "id": "q-DbDJj0vKYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classes from your selected superclass\n",
        "classes = os.listdir(os.path.join(data_dir, 'train', cat))\n",
        "print(\"Classes of\", cat)\n",
        "for c in classes:\n",
        "  print(\"-\",c)"
      ],
      "metadata": {
        "id": "K1BiKBzvvefe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "VF9DLiKHD1m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load raw image data into each class in class_data\n",
        "\n",
        "train_dir = os.path.join(data_dir, 'train', cat)\n",
        "test_dir = os.path.join(data_dir, 'test', cat)\n",
        "\n",
        "class_data = {}\n",
        "\n",
        "for c in classes:\n",
        "  class_train_dir = os.path.join(train_dir, c)\n",
        "  class_test_dir = os.path.join(test_dir, c)\n",
        "\n",
        "  train_files = [os.path.join(class_train_dir, f) for f in os.listdir(class_train_dir)]\n",
        "  test_files = [os.path.join(class_test_dir, f) for f in os.listdir(class_test_dir)]\n",
        "\n",
        "  class_data[c] = {'train' : train_files, 'test' : test_files}"
      ],
      "metadata": {
        "id": "cM2fso9XEE54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot an example image of each class\n",
        "\n",
        "fig, ax = plt.subplots(1,len(classes))\n",
        "for i, c in enumerate(classes):\n",
        "  im = ski.io.imread(class_data[c]['train'][0])\n",
        "  ax[i].imshow(im)\n",
        "  ax[i].set_title(f'{i}: {c}')\n",
        "  ax[i].axis('off')\n",
        "fig.show()\n",
        "\n",
        "print('Image shape:', im.shape)\n",
        "print('Train data size per class:', len(class_data[c]['train']))\n",
        "print('Test data size per class:', len(class_data[c]['test']))"
      ],
      "metadata": {
        "id": "MlhadHgoEFeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our dictionary into a nparray for training. Also, shuffle the data.\n",
        "\n",
        "X_train = [] # train images\n",
        "y_train = [] # train labels\n",
        "for i, c in enumerate(classes):\n",
        "  X_train.extend([ski.io.imread(filepath) for filepath in class_data[c]['train']])\n",
        "  y_train.extend([i for filepath in class_data[c]['train']]) # i = class index\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=0)\n",
        "\n",
        "\n",
        "_X_test = [] # test images\n",
        "_y_test = [] # test labels\n",
        "for i, c in enumerate(classes):\n",
        "  _X_test.extend([ski.io.imread(filepath) for filepath in class_data[c]['test']])\n",
        "  _y_test.extend([i for filepath in class_data[c]['test']])\n",
        "_X_test = np.array(_X_test)\n",
        "_y_test = np.array(_y_test)\n",
        "_X_test, _y_test = shuffle(_X_test, _y_test, random_state=0)"
      ],
      "metadata": {
        "id": "adl-9TIdYQZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shapes of Original Sets:')\n",
        "print('Training set:', X_train.shape)\n",
        "print('Testing set:', _X_test.shape)"
      ],
      "metadata": {
        "id": "rPogkZn-fzQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training-Validation-Testing Sets Split\n",
        "\n",
        "Right now, we have the train and test set. We need to reformat it for our SVM and include a validation set.\n",
        "\n",
        "Let's split the original test set to half test and half validation."
      ],
      "metadata": {
        "id": "t2VFVRCKX3D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please implement an even validation-test split\n",
        "# Hint: use train_test_split()\n",
        "X_val, X_test, y_val, y_test ="
      ],
      "metadata": {
        "id": "gdOnR6nof6CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shapes of Split Sets:')\n",
        "print('Training set:', X_train.shape)\n",
        "print('Validation set:', X_val.shape)\n",
        "print('Testing set:', X_test.shape)"
      ],
      "metadata": {
        "id": "tP4n9dZpjxMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if your images still match to your labels."
      ],
      "metadata": {
        "id": "-m_YlBTZltt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,5)\n",
        "for i in range(5):\n",
        "  ax[i].imshow(X_val[i])\n",
        "  ax[i].set_title(f'{i}: {classes[y_val[i]]}')\n",
        "  ax[i].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "KWc2obqNlsru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten and normalize data for SVM"
      ],
      "metadata": {
        "id": "3QOCDE2LmGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten and normalize data as input for SVM\n",
        "# (num_img, h, w, channels) --> (num_img, num_features)\n",
        "\n",
        "_, image_height, image_width, image_channels = X_train.shape\n",
        "\n",
        "X_train_flat = np.array(X_train).reshape(-1, image_height * image_width * image_channels).astype('float') / 255\n",
        "X_val_flat = np.array(X_val).reshape(-1, image_height * image_width * image_channels).astype('float') / 255\n",
        "X_test_flat = np.array(X_test).reshape(-1, image_height * image_width * image_channels).astype('float') / 255"
      ],
      "metadata": {
        "id": "Pq9FquOZl949"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shapes of Flattened Sets:')\n",
        "print('Training set:', X_train_flat.shape)\n",
        "print('Validation set:', X_val_flat.shape)\n",
        "print('Testing set:', X_test_flat.shape)"
      ],
      "metadata": {
        "id": "wZqbpVwGmgoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Your Support Vector Machines (SVM)"
      ],
      "metadata": {
        "id": "cP8cwzxlnGjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate SVM model\n",
        "# Hint: use SVC()\n",
        "model ="
      ],
      "metadata": {
        "id": "gwFMT9F9m2qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "# Hint: use model.fit()\n"
      ],
      "metadata": {
        "id": "Wx7aajmvjENo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions for validation or testing set\n",
        "# Hint: use model.predict()\n",
        "y_pred ="
      ],
      "metadata": {
        "id": "G0FEL-F_jHJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "# Hint: use classification_report()\n"
      ],
      "metadata": {
        "id": "bLWAlVsljLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some results!"
      ],
      "metadata": {
        "id": "V7y_eSnm3qc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 5\n",
        "random_indices = np.random.choice(X_test.shape[0], num_samples, replace=False)\n",
        "fig, ax = plt.subplots(1,num_samples)\n",
        "for i in range(num_samples):\n",
        "  ax[i].imshow(X_test[random_indices[i]])\n",
        "  ax[i].set_title(f'{i}: {classes[y_pred[random_indices[i]]]}')\n",
        "  ax[i].axis('off')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "aNzwxQyRr1s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Work:\n",
        "* Try to experiment with different kernels.\n",
        "* Tune the hyperparameters. The performance of SVM is sensitive to the C parameter (regularization) and the gamma parameter (for the RBF kernel).\n",
        "* **Use grid search (GridSearchCV) or random search (RandomizedSearchCV) to find the best values for hyperparameters.**\n",
        "* Perform feature selection/engineering on the SVM input.\n",
        "* Fit SVM on a different public dataset or your custom dataset.\n",
        "* Build a CNN on this dataset.\n"
      ],
      "metadata": {
        "id": "d0Rrm64K5S7L"
      }
    }
  ]
}